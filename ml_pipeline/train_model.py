#!/usr/bin/env python3
"""
ExoAI Hunter - Model Training Script
Advanced training pipeline for achieving >95% accuracy on NASA exoplanet datasets
"""

import os
import sys
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
import argparse
import logging
from pathlib import Path

# Import our custom modules
from .data_processor import ExoplanetDataProcessor
from .exoplanet_model import ExoAIHunterModel, create_exoai_model

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class ExoAITrainer:
    """
    Advanced training pipeline for ExoAI Hunter models
    Implements NASA challenge requirements for >95% accuracy
    """
    
    def __init__(self, config_path=None):
        self.config = self.load_config(config_path)
        self.project_root = Path(__file__).parent.parent
        self.data_dir = self.project_root / "data"
        self.models_dir = self.project_root / "models"
        self.results_dir = self.project_root / "results"
        
        # Create directories
        for directory in [self.data_dir, self.models_dir, self.results_dir]:
            directory.mkdir(exist_ok=True)
        
        # Initialize components
        self.data_processor = None
        self.models = {}
        self.results = {}
        
    def load_config(self, config_path):
        """Load training configuration"""
        default_config = {
            "models": {
                "cnn": {
                    "epochs": 100,
                    "batch_size": 32,
                    "learning_rate": 0.001,
                    "sequence_length": 1000
                },
                "transformer": {
                    "epochs": 80,
                    "batch_size": 16,
                    "learning_rate": 0.0005,
                    "d_model": 128,
                    "num_heads": 8,
                    "num_layers": 4
                }
            },
            "training": {
                "validation_split": 0.2,
                "test_split": 0.1,
                "cross_validation_folds": 5,
                "random_state": 42,
                "target_accuracy": 0.95
            },
            "data": {
                "missions": ["kepler", "k2", "tess"],
                "min_data_points": 100,
                "preprocessing": {
                    "normalize": True,
                    "detrend": True,
                    "remove_outliers": True
                }
            }
        }
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r') as f:
                user_config = json.load(f)
                # Merge with default config
                default_config.update(user_config)
        
        return default_config
    
    def load_nasa_datasets(self):
        """
        Load and combine NASA exoplanet datasets
        """
        logger.info("üõ∞Ô∏è  Loading NASA datasets...")
        
        datasets = {}
        total_objects = 0
        
        for mission in self.config["data"]["missions"]:
            logger.info(f"Loading {mission.upper()} dataset...")
            
            # Initialize data processor for mission
            processor = ExoplanetDataProcessor(mission_type=mission)
            
            # Load dataset (would be actual NASA data in production)
            dataset_file = self.data_dir / f"{mission}_objects.csv"
            
            if not dataset_file.exists():
                logger.info(f"Creating sample {mission.upper()} dataset...")
                self.create_sample_dataset(mission, dataset_file)
            
            df = processor.load_nasa_dataset(dataset_file)
            if df is not None:
                datasets[mission] = df
                total_objects += len(df)
                logger.info(f"‚úÖ Loaded {len(df)} objects from {mission.upper()}")
        
        logger.info(f"üìä Total objects loaded: {total_objects}")
        return datasets
    
    def create_sample_dataset(self, mission, filepath):
        """Create sample dataset for demonstration"""
        np.random.seed(42)
        
        # Dataset sizes based on real NASA data
        sizes = {"kepler": 4284, "k2": 1203, "tess": 6341}
        n_samples = sizes.get(mission, 1000)
        
        data = []
        for i in range(n_samples):
            # Generate realistic exoplanet parameters
            period = np.random.lognormal(2, 1.5)  # Log-normal distribution
            radius = np.random.lognormal(0, 0.8)  # Log-normal for radius
            depth = np.random.uniform(10, 10000)  # Transit depth in ppm
            duration = np.random.uniform(0.5, 24)  # Duration in hours
            
            # Assign disposition with realistic proportions
            if mission == "kepler":
                disposition = np.random.choice(
                    ['CONFIRMED', 'CANDIDATE', 'FALSE POSITIVE'], 
                    p=[0.62, 0.29, 0.09]
                )
            elif mission == "k2":
                disposition = np.random.choice(
                    ['CONFIRMED', 'CANDIDATE', 'FALSE POSITIVE'], 
                    p=[0.40, 0.45, 0.15]
                )
            else:  # TESS
                disposition = np.random.choice(
                    ['CONFIRMED', 'CANDIDATE', 'FALSE POSITIVE'], 
                    p=[0.45, 0.44, 0.11]
                )
            
            data.append({
                f'{mission}_id': f'{mission.upper()}-{i+1:05d}',
                'disposition': disposition,
                'period': period,
                'radius': radius,
                'depth': depth,
                'duration': duration,
                'ra': np.random.uniform(0, 360),
                'dec': np.random.uniform(-90, 90),
                'magnitude': np.random.uniform(8, 16),
                'temperature': np.random.uniform(200, 3000),
                'discovery_year': np.random.choice(range(2009, 2025))
            })
        
        df = pd.DataFrame(data)
        df.to_csv(filepath, index=False)
        logger.info(f"‚úÖ Created sample dataset: {filepath}")
    
    def prepare_training_data(self, datasets):
        """
        Prepare and preprocess training data from all missions
        """
        logger.info("üîß Preparing training data...")
        
        all_light_curves = []
        all_labels = []
        all_metadata = []
        
        for mission, df in datasets.items():
            logger.info(f"Processing {mission.upper()} data...")
            
            processor = ExoplanetDataProcessor(mission_type=mission)
            
            for idx, row in df.iterrows():
                try:
                    # Generate synthetic light curve based on disposition
                    if row['disposition'] == 'CONFIRMED':
                        light_curve = self.generate_confirmed_light_curve(row)
                    elif row['disposition'] == 'CANDIDATE':
                        light_curve = self.generate_candidate_light_curve(row)
                    else:  # FALSE POSITIVE
                        light_curve = self.generate_false_positive_light_curve(row)
                    
                    # Preprocess light curve
                    processed_curve = processor.preprocess_light_curve(light_curve)
                    
                    # Extract features
                    features = processor.extract_features(
                        processed_curve, 
                        period=row.get('period')
                    )
                    
                    # Ensure consistent length
                    target_length = self.config["models"]["cnn"]["sequence_length"]
                    if len(processed_curve) >= target_length:
                        processed_curve = processed_curve[:target_length]
                    else:
                        # Pad with median value
                        median_val = np.median(processed_curve)
                        padding = np.full(target_length - len(processed_curve), median_val)
                        processed_curve = np.concatenate([processed_curve, padding])
                    
                    all_light_curves.append(processed_curve)
                    all_labels.append(row['disposition'])
                    all_metadata.append({
                        'mission': mission,
                        'object_id': row.get(f'{mission}_id', f'{mission}_{idx}'),
                        'features': features
                    })
                    
                except Exception as e:
                    logger.warning(f"Error processing {mission} object {idx}: {e}")
                    continue
        
        # Convert to numpy arrays
        X = np.array(all_light_curves)
        y = np.array(all_labels)
        
        # Encode labels
        from sklearn.preprocessing import LabelEncoder
        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(y)
        
        logger.info(f"‚úÖ Prepared {len(X)} training samples")
        logger.info(f"   Classes: {list(label_encoder.classes_)}")
        logger.info(f"   Class distribution: {np.bincount(y_encoded)}")
        
        return X, y_encoded, label_encoder, all_metadata
    
    def generate_confirmed_light_curve(self, row):
        """Generate realistic confirmed exoplanet light curve"""
        length = np.random.randint(800, 1200)
        time = np.arange(length)
        
        # Base stellar flux with noise
        flux = np.ones(length) + 0.01 * np.random.normal(0, 1, length)
        
        # Add stellar variability
        flux += 0.005 * np.sin(2 * np.pi * time / 100)
        
        # Add clear transit signal
        period = row.get('period', 50)
        depth = row.get('depth', 1000) / 1e6  # Convert ppm to fraction
        duration = row.get('duration', 4)
        
        # Calculate transit positions
        transit_points = int(duration * length / (period * 24))  # Rough approximation
        transit_period_points = int(period * 2)  # Points per period
        
        for i in range(0, length, transit_period_points):
            if i + transit_points < length:
                flux[i:i+transit_points] -= depth
        
        return flux
    
    def generate_candidate_light_curve(self, row):
        """Generate candidate exoplanet light curve (weaker signal)"""
        length = np.random.randint(600, 1000)
        time = np.arange(length)
        
        # Base stellar flux with more noise
        flux = np.ones(length) + 0.015 * np.random.normal(0, 1, length)
        
        # Add stellar variability
        flux += 0.008 * np.sin(2 * np.pi * time / 80)
        
        # Add weak transit-like signal
        period = row.get('period', 75)
        depth = row.get('depth', 500) / 1e6 * 0.7  # Weaker signal
        duration = row.get('duration', 3)
        
        transit_points = int(duration * length / (period * 24))
        transit_period_points = int(period * 1.8)
        
        for i in range(0, length, transit_period_points):
            if i + transit_points < length:
                flux[i:i+transit_points] -= depth
        
        return flux
    
    def generate_false_positive_light_curve(self, row):
        """Generate false positive light curve (no transit signal)"""
        length = np.random.randint(700, 1100)
        time = np.arange(length)
        
        # Base stellar flux with significant variability
        flux = np.ones(length) + 0.02 * np.random.normal(0, 1, length)
        
        # Add multiple stellar variability components
        flux += 0.01 * np.sin(2 * np.pi * time / 60)
        flux += 0.005 * np.sin(2 * np.pi * time / 150)
        flux += 0.003 * np.sin(2 * np.pi * time / 300)
        
        # Add occasional flares or instrumental effects
        num_events = np.random.poisson(2)
        for _ in range(num_events):
            event_start = np.random.randint(0, length - 50)
            event_length = np.random.randint(5, 30)
            flux[event_start:event_start+event_length] += np.random.uniform(0.002, 0.01)
        
        return flux
    
    def train_models(self, X_train, y_train, X_val, y_val):
        """
        Train multiple model architectures
        """
        logger.info("üöÄ Starting model training...")
        
        input_shape = X_train.shape[1:]
        num_classes = len(np.unique(y_train))
        
        # Train CNN model
        logger.info("Training CNN model...")
        cnn_model = create_exoai_model('cnn', num_classes)
        cnn_model.create_model(input_shape, 'cnn_primary')
        
        cnn_history = cnn_model.train_model(
            X_train, y_train, X_val, y_val,
            model_name='cnn_primary',
            epochs=self.config["models"]["cnn"]["epochs"],
            batch_size=self.config["models"]["cnn"]["batch_size"]
        )
        
        self.models['cnn'] = cnn_model
        
        # Train Transformer model
        logger.info("Training Transformer model...")
        transformer_model = create_exoai_model('transformer', num_classes)
        
        # Reshape data for transformer (add feature dimension)
        X_train_transformer = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
        X_val_transformer = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)
        
        transformer_model.create_model(X_train_transformer.shape[1:], 'transformer_primary')
        
        transformer_history = transformer_model.train_model(
            X_train_transformer, y_train, X_val_transformer, y_val,
            model_name='transformer_primary',
            epochs=self.config["models"]["transformer"]["epochs"],
            batch_size=self.config["models"]["transformer"]["batch_size"]
        )
        
        self.models['transformer'] = transformer_model
        
        logger.info("‚úÖ Model training completed!")
        
        return {
            'cnn': cnn_history,
            'transformer': transformer_history
        }
    
    def cross_validate_models(self, X, y, cv_folds=5):
        """
        Perform cross-validation to ensure robust performance
        """
        logger.info(f"üîÑ Starting {cv_folds}-fold cross-validation...")
        
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        cv_results = {}
        
        for model_name, model in self.models.items():
            logger.info(f"Cross-validating {model_name} model...")
            
            fold_scores = []
            
            for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
                logger.info(f"  Fold {fold + 1}/{cv_folds}")
                
                X_train_fold, X_val_fold = X[train_idx], X[val_idx]
                y_train_fold, y_val_fold = y[train_idx], y[val_idx]
                
                # Reshape for transformer if needed
                if model_name == 'transformer':
                    X_train_fold = X_train_fold.reshape(X_train_fold.shape[0], X_train_fold.shape[1], 1)
                    X_val_fold = X_val_fold.reshape(X_val_fold.shape[0], X_val_fold.shape[1], 1)
                
                # Create new model for this fold
                input_shape = X_train_fold.shape[1:]
                num_classes = len(np.unique(y))
                
                fold_model = create_exoai_model(model_name, num_classes)
                fold_model.create_model(input_shape, f'{model_name}_fold_{fold}')
                
                # Train on fold
                fold_model.train_model(
                    X_train_fold, y_train_fold, X_val_fold, y_val_fold,
                    model_name=f'{model_name}_fold_{fold}',
                    epochs=30,  # Reduced epochs for CV
                    batch_size=32
                )
                
                # Evaluate
                fold_metrics = fold_model.evaluate_model(X_val_fold, y_val_fold, f'{model_name}_fold_{fold}')
                fold_scores.append(fold_metrics['accuracy'])
                
                logger.info(f"    Fold {fold + 1} accuracy: {fold_metrics['accuracy']:.4f}")
            
            cv_results[model_name] = {
                'mean_accuracy': np.mean(fold_scores),
                'std_accuracy': np.std(fold_scores),
                'fold_scores': fold_scores
            }
            
            logger.info(f"‚úÖ {model_name} CV results:")
            logger.info(f"   Mean accuracy: {cv_results[model_name]['mean_accuracy']:.4f} ¬± {cv_results[model_name]['std_accuracy']:.4f}")
        
        return cv_results
    
    def create_ensemble_model(self, X_train, y_train, X_val, y_val):
        """
        Create ensemble model combining CNN and Transformer
        """
        logger.info("ü§ñ Creating ensemble model...")
        
        if 'cnn' not in self.models or 'transformer' not in self.models:
            logger.error("Both CNN and Transformer models must be trained before creating ensemble")
            return None
        
        # Get predictions from both models
        cnn_pred = self.models['cnn'].models['cnn_primary'].predict(X_val)
        
        X_val_transformer = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)
        transformer_pred = self.models['transformer'].models['transformer_primary'].predict(X_val_transformer)
        
        # Simple ensemble averaging
        ensemble_pred = (cnn_pred + transformer_pred) / 2
        ensemble_classes = np.argmax(ensemble_pred, axis=1)
        
        # Calculate ensemble accuracy
        ensemble_accuracy = np.mean(ensemble_classes == y_val)
        
        logger.info(f"üéØ Ensemble accuracy: {ensemble_accuracy:.4f}")
        
        if ensemble_accuracy >= self.config["training"]["target_accuracy"]:
            logger.info("üèÜ TARGET ACHIEVED: Ensemble model exceeds 95% accuracy!")
        
        return {
            'accuracy': ensemble_accuracy,
            'predictions': ensemble_pred
        }
    
    def generate_report(self, cv_results, ensemble_results):
        """
        Generate comprehensive training report
        """
        logger.info("üìã Generating training report...")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'config': self.config,
            'cross_validation_results': cv_results,
            'ensemble_results': ensemble_results,
            'nasa_challenge_compliance': {
                'target_accuracy': self.config["training"]["target_accuracy"],
                'achieved': False,
                'best_model': None,
                'best_accuracy': 0
            }
        }
        
        # Determine best performing model
        best_accuracy = 0
        best_model = None
        
        for model_name, results in cv_results.items():
            if results['mean_accuracy'] > best_accuracy:
                best_accuracy = results['mean_accuracy']
                best_model = model_name
        
        if ensemble_results and ensemble_results['accuracy'] > best_accuracy:
            best_accuracy = ensemble_results['accuracy']
            best_model = 'ensemble'
        
        report['nasa_challenge_compliance']['best_model'] = best_model
        report['nasa_challenge_compliance']['best_accuracy'] = best_accuracy
        report['nasa_challenge_compliance']['achieved'] = best_accuracy >= self.config["training"]["target_accuracy"]
        
        # Save report
        report_file = self.results_dir / f'training_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"üìÑ Report saved: {report_file}")
        
        # Print summary
        logger.info("üéØ TRAINING SUMMARY:")
        logger.info(f"   Target accuracy: {self.config['training']['target_accuracy'] * 100:.1f}%")
        logger.info(f"   Best model: {best_model}")
        logger.info(f"   Best accuracy: {best_accuracy * 100:.2f}%")
        logger.info(f"   Target achieved: {'‚úÖ YES' if report['nasa_challenge_compliance']['achieved'] else '‚ùå NO'}")
        
        return report
    
    def run_full_training_pipeline(self):
        """
        Execute the complete training pipeline
        """
        logger.info("üöÄ Starting ExoAI Hunter training pipeline...")
        logger.info("üéØ Target: >95% accuracy for NASA Space Apps Challenge 2025")
        
        try:
            # Load datasets
            datasets = self.load_nasa_datasets()
            
            # Prepare training data
            X, y, label_encoder, metadata = self.prepare_training_data(datasets)
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, 
                test_size=self.config["training"]["test_split"],
                stratify=y,
                random_state=self.config["training"]["random_state"]
            )
            
            X_train, X_val, y_train, y_val = train_test_split(
                X_train, y_train,
                test_size=self.config["training"]["validation_split"],
                stratify=y_train,
                random_state=self.config["training"]["random_state"]
            )
            
            logger.info(f"üìä Data split:")
            logger.info(f"   Training: {len(X_train)} samples")
            logger.info(f"   Validation: {len(X_val)} samples")
            logger.info(f"   Test: {len(X_test)} samples")
            
            # Train models
            training_histories = self.train_models(X_train, y_train, X_val, y_val)
            
            # Cross-validation
            cv_results = self.cross_validate_models(X, y, self.config["training"]["cross_validation_folds"])
            
            # Create ensemble
            ensemble_results = self.create_ensemble_model(X_train, y_train, X_val, y_val)
            
            # Generate report
            report = self.generate_report(cv_results, ensemble_results)
            
            # Save models
            for model_name, model in self.models.items():
                model.save_model()
            
            logger.info("üéâ Training pipeline completed successfully!")
            
            return report
            
        except Exception as e:
            logger.error(f"‚ùå Training pipeline failed: {e}")
            raise

def main():
    """Main training function"""
    parser = argparse.ArgumentParser(description='ExoAI Hunter Model Training')
    parser.add_argument('--config', type=str, help='Path to configuration file')
    parser.add_argument('--verbose', action='store_true', help='Verbose logging')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Initialize trainer
    trainer = ExoAITrainer(config_path=args.config)
    
    # Run training pipeline
    report = trainer.run_full_training_pipeline()
    
    # Print final results
    if report['nasa_challenge_compliance']['achieved']:
        print("\nüèÜ SUCCESS: ExoAI Hunter has achieved >95% accuracy!")
        print("üöÄ Ready for NASA Space Apps Challenge 2025!")
    else:
        print(f"\n‚ö†Ô∏è  Target not reached. Best accuracy: {report['nasa_challenge_compliance']['best_accuracy']*100:.2f}%")
        print("üîÑ Consider adjusting hyperparameters or model architecture.")

if __name__ == "__main__":
    main()
